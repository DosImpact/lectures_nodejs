- [섹션 4 \[기초편\] 컨트롤러](#섹션-4-기초편-컨트롤러)
- [1. Replication Controller, ReplicaSet](#1-replication-controller-replicaset)
  - [컨트롤러 목적](#컨트롤러-목적)
  - [Template, Replicas, Selector](#template-replicas-selector)
  - [실습 - ReplicaSet 1](#실습---replicaset-1)
  - [실습 - ReplicaSet matchLabels](#실습---replicaset-matchlabels)
- [2. Deployment - Recreate, RollingUpdate](#2-deployment---recreate-rollingupdate)
  - [ReCreate, RollingUpdate, BlueGreen, Canary](#recreate-rollingupdate-bluegreen-canary)
  - [Deployment - Recreate](#deployment---recreate)
  - [Deployment - RollingUpdate](#deployment---rollingupdate)
  - [실습 - 1. ReCreate](#실습---1-recreate)
  - [실습 - 2. RollingUpdate](#실습---2-rollingupdate)
  - [실습 - 3. Blue/Green](#실습---3-bluegreen)
- [3. DaemonSet, Job, CronJob](#3-daemonset-job-cronjob)
  - [DaemonSet 구성](#daemonset-구성)
  - [Job 구성](#job-구성)
  - [CronJob 구성](#cronjob-구성)
  - [실습 - 데몬셋을 각노드에 올려, NodeIP:hostPort로 접근 해보기](#실습---데몬셋을-각노드에-올려-nodeiphostport로-접근-해보기)
- [실습 - 노드를 선택해서 데몬셋 생성하기](#실습---노드를-선택해서-데몬셋-생성하기)
  - [실습 - Job](#실습---job)
  - [실습 - CronJob](#실습---cronjob)
  - [실습 - CronJob concurrencyPolicy](#실습---cronjob-concurrencypolicy)

# 섹션 4 [기초편] 컨트롤러

# 1. Replication Controller, ReplicaSet

목적 : 파드를 직접 하나씩 생성하지 않고, 원하는 만큼 파드를 줄이고 늘릴 수 있도록 컨트롤 하고 싶을 때 사용 

- 1. Replication Controller = Template + Replicas
- 2. ReplicaSet = Template + Replicas + Selector
- Replication Controller은 대신 ReplicaSet로 대처 되었다. 
- ReplicaSet으로 업데이트 가능 하다.

Template 생성할 파드 
Replicas 몇개의 파드 만들건지?
Selector 생성한 파드 선택자.


## 컨트롤러 목적

컨트롤러는 서비스를 운영하는데 큰 도움을 준다.  

1.  Auto Healing 
Node1 장애 > Pod1 죽음 - Node2에 새로운 Pod를 만들어 준다.
- ReplicationController, ReplicaSet, StatusfulSet, DaemonSet

2. Software Update 
한번에 Pod의 버전을 업그레이드 및 롤백을 해준다.
- Deployment

3. AutoScaling
Pod1의 자원이 부족하면 , Pod2를 만들어서 부하를 분산시켜 준다.
- HPA

4.  Job
특정 순간에 작업이 필요하면, Pod를 만들고 해당 작업을 수행 후 삭제한다.  
효율적인 자원을 사용하게 된다.
- CronJob, Job

---


## Template, Replicas, Selector

1. Template

- 컨트롤러에서 생성할 파드의 원본을 template에 적어준다.
- selector로 생성될 파드들과 연결시킬 수 있다. 
- Pod가 죽으면 template을 바탕으로 Pod를 복제해서 만들게 된다.  
- 이러한 특성으로, template의 버전을 업데이트해서 Pod를 교체할 수 있다.

2. Replicas

- 명시된 Replicas 만큼 Pod를 만들어 준다.
- 3개라면 3개 Pod까지 만들어 준다.
- 개수를 늘리고 줄이므로써 스케일링이 가능하다.  
- Template + Replicas을 동시에 명시해서, Replicas 갯수만큼 template으로 만든다.   

3. Selector

- ReplicaSet에만 있는 기능
- Replication Controller(기존의 방식)은 selector로 pod를 선택했는데, 좀 더 상세한 설정으로 pod를 선택할 수 있다.
- 이는 2가지 필드로 가능하다.
- matchLabels: 기존의 방식의 설렉터 작성  
- matchExpressions : 특정 key 혹은 key-value 값을 선택 가능 
  - Exists : 특정키를 가진 Pod들을 선택하고 싶을 때  
  - DoesNotExist : Exists 반대 로직  
  - In : 특정키 및 특정값이 존재하는 Pod들을 선택하고 싶을 때   
  - NotIn : In 반대 로직   


## 실습 - ReplicaSet 1

리플리케이션 컨트롤러는 deprecated 됐고 그 기능이 그대로 리플리카셋에도 있다.

```
# 1. pod를 하나 만든다.
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    type: web
spec:
  containers:
  - name: container
    image: kubetm/app:v1
  terminationGracePeriodSeconds: 0
---
# 2. ReplicaSet을 하나 만든다.
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 1
  selector:
    matchLabels:
      type: web
  template:
    metadata:
      name: pod1
      labels:
        type: web
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0

# 3.
- 이미 존재하는 Pod까지 포함하여 replica를 만들어준다.
- 기본적으로 pod가 30초 뒤에 지워지도록 설정되어 있다.
- terminationGracePeriodSeconds:0으로 바로 지워지게 만든다.

# 4.대시보드 > 래플리카셋에 들어가서, scale을 하나 늘려볼 수 있다.
- template의 pod이름은 무시하고, 새로운 이름의 pod로 만들어진다.

# 5. Pod를 삭제하면, 다시 Pod가 만들어진다.

# 6. 버전 업데이트
- replicaSet > template의 이미지 버전을 올리기 
- 기존의 Pod를 직접 지워주면 버전이 업데이트 된다.
---
...
    spec:
      containers:
        - name: container
          image: kubetm/app:v2
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File

# 7.
- 컨트롤러를 삭제하면, 연결된 파드들도 모두 지워진다.
- ( commend를 통해 pod지우는것을 막을 수 있다. )
- kubectl delete replicationcontrollers replication1 --cascade=false

```

## 실습 - ReplicaSet matchLabels

- ReplicaSet의 matchExpressions은 파드들을 세부적으로 컨트롤 할 때 사용한다.  
- 그보다 pod의 matchExpressions을 통해 특정 노드에 파드를 생성하도록 컨트롤 하는 용도로 자주 사용한다. 

```
# matchLabels을 포함한 ReplicaSet

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 1
  selector:
    matchLabels:
      type: web
      ver: v1
    matchExpressions:
    - {key: type, operator: In, values: [web]}
    - {key: ver, operator: Exists}
  template:
    metadata:
      labels:
        type: web
        ver: v1
        location: dev
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
---
#2.
- 셀렉터 있는 내용이 이 템플렛에 있는 라벨의 내용에 포함이 돼야 한다.
- selector matchLabels < (더 큰 범위의 라벨들) template labels
- matchExpressions도 마찬가지의 논리가 적용된다.


---
#2.1  matchLabels < template의 labels 이므로 생성 가능
spec:
  replicas: 1
  selector:
    matchLabels:
      type: web
      ver: v1
  template:
    metadata:
      labels:
        type: web
        ver: v1
        location: dev
---
#2.2 matchExpressions의 ver관련된 내용이 template의 labels에 없어서 오류

spec:
  replicas: 1
  selector:
    matchExpressions:
    - {key: type, operator: In, values: [web]}
    - {key: ver, operator: Exists}
  template:
    metadata:
      labels:
        type: web
        location: dev
```


# 2. Deployment - Recreate, RollingUpdate

Deployment : 서비스가 운영중인데 재배포를 할때 도움을 주는 컨트롤러 이다.  
- Deployment는 Replica를 포함하여 정의를 한다.

## ReCreate, RollingUpdate, BlueGreen, Canary

1. ReCreate

- Deployment를 만들면 V1 Pod들이 만들어진다. 
- V1 Pod를 삭제해서 downtime을 가진다.  
- 이후 V2 Pod를 만들게 된다. 
- 자원 사용량은 늘어나지 않지만, downtime이 발생한다.

2. Rolling Update  

- Deployment를 만들면 V1 Pod들이 만들어진다. 
- V2 파드를 1개 만들고, V1 Pod 1개를 삭제한다. - 순간적으로 Pod 3개가 존재하고 이때 V1,V2모두 트래픽이 유입된다.  
- 위 과정을 반복해서 V2로 완전히 바뀐다. 
- 추가 자원사용량이 있지만 / zero downtime 이다
- V1,V2모두 트래픽이 유입된다

3. BlueGreen  

- Pod V1을 가진 Controller를 만들고 Service는 V1을 바라본다.  
- Pod V2을 가진 Controller를 만들고 Service는 V2을 바라보도록 한다.
- 자원사용량이 2배가 된다 / zero downtime 이다.
- V1,V2 트래픽이 분리된다.
- 가장많이 사용되는 전통적인 방법이다.

4. Canary

방법1 - 일부 트래픽을 V2로 주고 싶을 때
asis) Service > Controller = Pod V1 + Pod V1
tobe) Service > Controller = Pod V1 + Pod V1 + Controller = Pod V2 
- (일부 트래픽이 Pod V2에 유입되어 테스트)

방법2 - 특정 url을 통해서 V2로 트래픽을 주고 싶을 때
asis) Service > Controller = Pod V1 + Pod V1
tobe) Service1 > Controller = Pod V1 + Pod V1 | Service2 > Controller = Pod V2 
- IngressControlelr가 Servce1,2에 트래픽 분산
- 특정 url에 V2 서비스로 연결시킬 수 있다. 예를들어 미국 유저만 V2 연동

추가 자원사용량이 있지만 / zero downtime 이다. / V2 테스트 및 트래픽이 분리된다.

## Deployment - Recreate 

ReCreate : strategy 타입이 Recreate인 Deployment을 만들어서 구현 가능.


생성단계
- Deployment에 ReplicaSet의 내용이 포함된다. 즉 template, replicas, selector 내용을 포함
- Deployment에서 ReplicaSet을 만들게 되며, ReplicaSet의 본연의 동작처럼 Pod들이 만들어진다.
- 만들어둔 Service이 생성된 Pod와 연동이 된다.

업데이트 단계
- Deployment의 template의 파드들을 V2로 변경
- V1의 ReplicaSet의 replica = 0으로 만들어 파드들을 제거한다. (Downtime 시작)
- V2의 ReplicaSet가 하나 더 만들어진다. 이에 따라 Pod들도 V2로 ReCreate 된다. (Downtime 끝)  
- revisionHistoryLimit : ReplicaSet의 히스토리를 몇개까지 보관할지 결정하는 값 
- V1의 ReplicaSet은 replica=0 으로 히스토리가 남아 있다. 이를 몇개 유지할지 결정값


## Deployment - RollingUpdate


RollingUpdate(default)  

생성단계는 위와 동일

업데이트 단계
- ReplicaSet의 replica갯수를 점진적으로 증/감 시키면서 배포가 된다.
- Deployment - ReplicaSet V1 (2 = v1Pod 개수)
- Deployment - ReplicaSet V1 (2) + ReplicaSet V2 (0 = v2Pod 개수)
- Deployment - ReplicaSet V1 (1) + ReplicaSet V2 (1)
- Deployment - ReplicaSet V1 (0) + ReplicaSet V2 (2)
  - minReadySeconds: replica를 조절하는 시간간격
  - ReplicaSet V2는 V2 Pod로만 연결되도록 내부적인 매핑과정이 있다. 

## 실습 - 1. ReCreate

```
#1. ReCreate 타입의 Deployment를 생성

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  selector:
    matchLabels:
      type: app
  replicas: 2
  strategy:
    type: Recreate # ReCreate
  revisionHistoryLimit: 1
  template:
    metadata:
      labels:
        type: app
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 10
---
# 2 서비스 생성 후 서비스의 클러스터 IP를 알아내자. 
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    type: app
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080

---
# 3 아래 명령어로 1초마다 curl 명령어를 보내자.
while true; do curl 10.100.15.28:8080/version; sleep 1; done
---
# 4 업데이트
- deployment의 yaml 파일에서, template > 파드의 버전을 올리자.

spec:
  replicas: 2
  selector:
    matchLabels:
      type: app
  template:
    metadata:
      creationTimestamp: null
      labels:
        type: app
    spec:
      containers:
        - name: container
          image: kubetm/app:v2 # v1 > v2

# 5
curl 명령어에 대한 응답이 다음 과정으로 변했다.
- Version : v1
- curl: (7) Failed to connect to 10.100.15.28 port 8080 after 0 ms: Connection refused
- Version : v2
중간에 downtime이 발생했다.


# 6
kubectl -n nm-1 rollout undo deployment deployment-1 --to-revision=1
kubectl -n nm-1 rollout history deployment deployment-1

kubectl -n nm-1 rollout undo deployment deployment-1 --to-revision=2
kubectl -n nm-1 rollout history deployment deployment-1


```

## 실습 - 2. RollingUpdate

```
#1. 위와 동일한 방식으로 deployment을 생성한다.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-2
spec:
  selector:
    matchLabels:
      type: app2
  replicas: 2
  strategy:
    type: RollingUpdate
  minReadySeconds: 10
  template:
    metadata:
      labels:
        type: app2
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Service
metadata:
  name: svc-2
spec:
  selector:
    type: app2
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
---
while true; do curl 10.97.66.59:8080/version; sleep 1; done

#2. 업데이트도 동일하게 진행한다.
          image: kubetm/app:v2 # v1 > v2

#3. 
curl 명령어에 대한 응답이 다음 과정으로 변했다.
- Version : v1
- Version : v1 및 v2
- Version : v2

v1,v2에 모두 트래픽이 분산되다가, v2로 트래픽이 유입된다.


```

## 실습 - 3. Blue/Green

```
#1. blue/green 배포를 위해 ReplicaSet를 만들어 준다.
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 2
  selector:
    matchLabels:
      ver: v1
  template:
    metadata:
      name: pod1
      labels:
        ver: v1
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
---
# 서비스 생성
apiVersion: v1
kind: Service
metadata:
  name: svc-3
spec:
  selector:
    ver: v1
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
---
while true; do curl 10.106.138.31:8080/version; sleep 1; done

# 2. 업데이트
---
# 2.1 V2의 ReplicaSet를 하나 만들어서, Pod들을 대기 시킨다.
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 2
  selector:
    matchLabels:
      ver: v2
  template:
    metadata:
      name: pod1
      labels:
        ver: v2
    spec:
      containers:
      - name: container
        image: kubetm/app:v2
      terminationGracePeriodSeconds: 0
---
# 2.2 서비스의 selector를 v2로 변경하여 트래픽 유입을 확인한다.
  selector:
    ver: v2

# 2.3 남은 v1 ReplicaSet를 삭제


```

# 3. DaemonSet, Job, CronJob

DaemonSet
- 모든 노드에 Pod가 1개씩 만들어 진다.
- 노드의 자원과 상관없이 생성된다.
- 예) 프로메테우스, Logging - fluentd, Storage -  ClusterFS
- k8s에서 자체적으로 만드는 프록시 네트워크 파드

Job : 특정한 Job을 위해 생성되는 파드, 실행이 종료되면 Finish 상태의 파드로 남는다.
CronJob : Job들을 주기적으로 생성하는 역할
- usecase : DB백업, 주기적인 업데이트 체크, 메시징

## DaemonSet 구성

selector : pod 선택자
template : 생성할 pod
옵션 
- nodeSelector : 특정 노드에 다른 운영체제가 깔려있어서 실행이 불가능한 상황이면, 데몬셋 생성을 제외할 수 있다.
- hostPort : Node에 직접 Port가 생성되어 직접 Pod로 접근이 가능하다.


## Job 구성

selector : 설정없어도, 알아서 만들어진다.
template : 생성할 pod
옵션
- completions : 6개의 파드들을 순차적으로 실행할 예정 
- parallelism : 2개의 파드를 동시에 실행
- activeDeadlineSeconds : 30초 이상의 작업이 걸리면, Pod를 삭제후 작업 
- restartPolicy : Never, ㅒnFailure

## CronJob 구성

우리가 흔히 사용하는 cron 형식을 따른다.  
```  */1 * * * * : 1분에 1개씩 job을 만드는 cron  ```
- CronJob > Job이 만들어지고 Job에 명시된 내용이 실행된다.
옵션
- concurencyPolicy : allow 이전의 job에 상관없이 새로운 Job+Pod가 생성된다.
- concurencyPolicy : forbid 이전의 job이 실행중이면, Job 생성을 스킵한다.
- concurencyPolicy : Replace  이전의 job이 실행중이면, 기존 Job에 새로운 Pod를 연결시켜 이어간다.

## 실습 - 데몬셋을 각노드에 올려, NodeIP:hostPort로 접근 해보기

```
# DaemonSet 생성 및 hostPort로 Node에 포트연결
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-1
spec:
  selector:
    matchLabels:
      type: app
  template:
    metadata:
      labels:
        type: app
    spec:
      containers:
      - name: container
        image: kubetm/app
        ports:
        - containerPort: 8080
          hostPort: 18080
---
1. 각 노드에서 각각의 데몬 pod에 접근이 가능하다.
curl 192.168.49.2:18080/hostname
curl 192.168.49.3:18080/hostname
curl 192.168.49.4:18080/hostname

```
# 실습 - 노드를 선택해서 데몬셋 생성하기

```
#1. node에 라벨 추가하기
kubectl label nodes minikube-m02 os=centos
kubectl label nodes minikube-m03 os=ubuntu
---
#2. DaemonSet 생성 + nodeSelector 선택자

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-2
spec:
  selector:
    matchLabels:
      type: app
  template:
    metadata:
      labels:
        type: app
    spec:
      nodeSelector:
        os: centos
      containers:
      - name: container
        image: kubetm/app
        ports:
        - containerPort: 8080
---
3.
centos가 붙은 노드에만 데몬셋이 할당된다.

4.
node의 라벨을 centos 변경하면, 이를 감지하고 해당 노드에 데몬셋이 추가 된다.

5.
데몬셋Pod의 버전을 업데이트 하면 Rolling(기본) 업데이트가 된다.

```

## 실습 - Job

```
# 1. Job 만들기
# 20초 정도 걸리는 작업을 가정한다.

apiVersion: batch/v1
kind: Job
metadata:
  name: job-1
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: container
        image: kubetm/init
        command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
      terminationGracePeriodSeconds: 0
---
# 2. Job만들기 + activeDeadlineSeconds
apiVersion: batch/v1
kind: Job
metadata:
  name: job-2
spec:
  completions: 6
  parallelism: 2
  activeDeadlineSeconds: 30
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: container
        image: kubetm/init
        command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
      terminationGracePeriodSeconds: 0

3.
2개의 파드가 동시에 생성되며 작업을 시작하지만,
20초 이후 생성된 파드에서 job을 수행할 때 activeDeadlineSeconds에 걸려 job이 종료된다.
---
SuccessfulDelete
SuccessfulDelete
DeadlineExceeded
SuccessfulCreate
SuccessfulCreate
SuccessfulCreate
SuccessfulCreate

```
## 실습 - CronJob

```
#1. CronJob 생성하기
# 이는 1분마다 Job을 만드는 역학을 한다.

apiVersion: batch/v1
kind: CronJob
metadata:
  name: cron-job
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
          terminationGracePeriodSeconds: 0
---

# 2. 직접 job을 만들 수 있다.
# 어떤 크론잡에서, 어떤 job의 이름으로 만들건지?
kubectl create job --from=cronjob/cron-job cron-job-manual-001

# suspend 를 true로 만들면 일시적으로 job을 만들지 않는다.
kubectl patch cronjobs cron-job -p '{"spec" : {"suspend" : false }}'
```

## 실습 - CronJob concurrencyPolicy

```
#1. Forbid 정책을 가진 cronjob 생성

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cron-job-2
spec:
  schedule: "20,21,22 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 140; echo 'job end'"]
          terminationGracePeriodSeconds: 0

#2. 결과
그 전의 Job이 끝날때까지 Job이 만들어지지 않는다.

```