
- [Object - Pod](#object---pod)
  - [개념](#개념)
  - [실습 전 알아둬야할 사항](#실습-전-알아둬야할-사항)
  - [실습 - pod 올리기](#실습---pod-올리기)
  - [실습 - ReplicationController](#실습---replicationcontroller)
  - [실습 - label](#실습---label)
  - [실습 - Node Schedule](#실습---node-schedule)
- [Service - ClusterIP, NodePort, LoadBalancer](#service---clusterip-nodeport-loadbalancer)
  - [실습 - Service ClusterIP 파드가 IP변경되어도 Service IP로 접속 가능](#실습---service-clusterip-파드가-ip변경되어도-service-ip로-접속-가능)
  - [실습 - Service NodePort](#실습---service-nodeport)
  - [실습 - Service LoadBalancer](#실습---service-loadbalancer)

# Object - Pod 
https://kubetm.github.io/k8s/03-beginner-basic-resource/pod/

## 개념

컨테이너
- 파드안에는 여러 컨테이너 존재
- 컨테이너 끼리는 포트가 겹치면 안된다.
- 파드에는 고유의 IP주소가 할당되고 이는 인터널에서만 접근 가능하다. 외부에서 접근은 불가능하다.
- pod에 문제가 발생이 생기면 다시 만들어지고, 이때 IP는 다시 할당된다.

라벨
- key-value가 한쌍이다.
- Pod들을 분류하는데 사용된다. 뿐만 아니라 k8s 오브젝트에 모두 라벨을 달 수 있다.
- 예를들어 type:webapp 이며, profile이 dev인 pod만 고를 수 있다.
- 해시 태그를 달아둔것 같은 기능

노드 스케쥴
- 1. Pod를 배포할 노드를 직접 지정하는 방법
- 어떤 노드인지 명시함으로써 가능하다. nodeSelector 라는 필드를 사용
- 2. k8s의 스케쥴러가 판단해서 배포하는 방법
- 필요한 리소스의 양을 적어주면 된다. resources: requests,limit 등의 필드를 사용

## 실습 전 알아둬야할 사항

minikube 대시보드를 접속한다.
minikube 대시보드에서 blueprint yaml을 올릴 수 있다. (pod, service 등등 생성)
minikube 는 docker 컨테이너 이므로, 해당 bash에 들어가는것이 master node에 들어가는 것이다.
minikube 에서 pod의 bash로 접속이 가능하다.


## 실습 - pod 올리기

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container1
    image: kubetm/p8000
    ports:
    - containerPort: 8000
  - name: container2
    image: kubetm/p8080
    ports:
    - containerPort: 8080
```

```
minikube bash 접속 > Pod ip 알아내기 > 아래 명령어 수행


# curl 10.244.0.25:8000
containerPort : 8000

# curl 10.244.0.25:8080
containerPort : 8080
---
각 Pod의 컨테이너 접속해서 > 아래 명령어수행

root@pod-1:/# curl 0.0.0.0:8080
containerPort : 8080
root@pod-1:/# curl 0.0.0.0:8000
containerPort : 8000
---
만약 동일 Pod안에서 port가 충돌되면 하나의 파드에서 오류가 난다.
```

## 실습 - ReplicationController

아래 ReplicationController 을 통해서 만든 pod는 지우게 되면 다시 만들어지게 된다.

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: replication-1
spec:
  replicas: 1
  selector:
    app: rc
  template:
    metadata:
      name: pod-1
      labels:
        app: rc
    spec:
      containers:
      - name: container
        image: kubetm/init
```

## 실습 - label

아래 6개의 파드를 올린다.
- dev, prod 환경 각각 web-server-db를 올리자.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
    type: web
    lo: dev
spec:
  containers:
  - name: container
    image: kubetm/init
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  labels:
    type: db
    lo: dev
spec:
  containers:
  - name: container
    image: kubetm/init 
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
  labels:
    type: server
    lo: dev
spec:
  containers:
  - name: container
    image: kubetm/init  
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-4
  labels:
    type: web
    lo: prod
spec:
  containers:
  - name: container
    image: kubetm/init
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-5
  labels:
    type: db
    lo: prod
spec:
  containers:
  - name: container
    image: kubetm/init 
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-6
  labels:
    type: server
    lo: prod
spec:
  containers:
  - name: container
    image: kubetm/init  
```

아래 서비스는 web인 pod만 선택하는것을 확인할 수 있다.
```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    type: web
  ports:
  - port: 8080
```

아래 서비스는 prod 환경의 pod만 선택하는것을 확인할 수 있다.
```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-1-prod
spec:
  selector:
    lo: prod
  ports:
  - port: 8080
```

## 실습 - Node Schedule

실습하기 전에 minikube을 통해서 node를 하나 추가하자.

eg) 특정 노드를 명시해서 pod를 띄우는 blueprint
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
spec:
  nodeSelector:
    kubernetes.io/hostname: minikube-m02
  containers:
  - name: container
    image: kubetm/init
```

eg) 자원에 따라 스케쥴링 하여 pod를 띄우는 blueprint
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-4
spec:
  containers:
  - name: container
    image: kubetm/init
    resources:
      requests:
        memory: 2Gi
      limits:
        memory: 3Gi
```

# Service - ClusterIP, NodePort, LoadBalancer

Pod에도 IP가 있는데 서비스를 만들어 붙이는 이유는 ? 
Pod는 쉽게 죽고 재생성이 되도록 만들어진 객체이다. 그래서 IP가 수시로 변할 수 있다. 
그래서 Service가 항상 연결된 IP를 제공하도록 한다. 

ClusterIP : 클러스터 내에서만 접근이 가능한 IP, External에서 접근할 수 없다.ㅇ
- 여러개의 Pod를 연결해서 트래픽을 분산시켜준다.
- Service의 기본 타입은 ClusterIP 이다.
- 외부에서 접근이 불가능하고, 내부의 인가된 사용자만 사용이 가능하다. 내부 대시보드, Pod상태의 서비스 상태 디버깅 등

NodePort : 모든 노드에 동일한 포트를 오픈시켜서 서비스를 연결시켜준다.
- 이후 ClusterIP와 유사하게 동작한다.
- Pod가 있는 노드에만 nodePort가 열리는것이 아니다.!
- 그래서 목적지 파드가 없는 노드에서도 파드에 도달할 수 있다.
- 옵션 : externalTrafficPolicy:Local  > 서비스가 해당 노드안의 파드들까지만 트래픽을 전달해준다.
- 외부에서 접근 가능하나, 보안적으로 내부망 연결해서 접속하도록 한다.


Load Balancer
- NodePort의 성격을 그대로 가지고 있다.
- 하지만 별도로 플러그인을 설치를 해서, 클러스터 외부에서 로드밸런서를 만들어야 한다.
- 그래고 해당 로드밸런서에 IP 할당해서 클러스터 접근가능하도록 한다.
- 실제적으로 외부의 서비스를 노출 시키도록 하여, 내부의 IP를 노출시키지 않는다.

## 실습 - Service ClusterIP 파드가 IP변경되어도 Service IP로 접속 가능

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
     app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: minikube
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080    


서비스 ClusterIP로 접근하면 파드의 IP에 상관없이 응답이 온다.
>curl 10.111.143.65:9000/hostname
Hostname : pod-1

```

## 실습 - Service NodePort

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
     app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: minikube
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  labels:
     app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: minikube-m02
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: svc-2
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
    nodePort: 30000
  type: NodePort
  externalTrafficPolicy: Local
---
ClusterIP 처럼 접근이 가능하다.
>curl 10.111.143.65:9000/hostname
Hostname : pod-1
--- 
하지만 NodePort로 해당 노드에 IP로 찔러서 접근이 가능하다.
>curl localhost:30000/hostname
Hostname : pod-1
---
아래처럼 트래픽이 각 노드로 분산이 된다. 
노드1, 노드2에 동시에 30000번 포트가 열린것이고 각각의 노드의 파드로 트래픽을 분산시켜준다.
이는 노드1에서 접근해도 노드2의 Pod로도 트래픽을 보내준다.

>curl localhost:30000/hostname
Hostname : pod-1
>curl localhost:30000/hostname
Hostname : pod-2

```

## 실습 - Service LoadBalancer

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-3
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
  type: LoadBalancer
---

서비스를 보면 ExternalIP가 아직 Pending 상태로 외부 아이피 할당을 기다리고 있다.

> kubectl get service
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
svc-3            LoadBalancer   10.110.41.44    <pending>     9000:30929/TCP   59s

```